{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a model which uses pytorch's Module (nn.Module) to create an autoencoder. Autoencoders are used to replicate the input we feed into the network by encoding and decoding the input multiple times.\n",
    "Refer to SAE.py for the implementation used here.  \n",
    "\n",
    "We start by importing all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.nn.parallel as parallel\n",
    "from torch.autograd import Variable\n",
    "from SAE import SAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy and pndas are going to help us out with arithmetic and for handling dataframes, respectively. Torch will be for creating out autoencoder, training it and testing the results.\n",
    "And the last line imports the Stacked AutoEncoder class we’ve created. If we prefer we can move the training logic into the class itself in the form of class methods but in order to experiment with optimizers and training logic, it can be left here.\n",
    "\n",
    "Before we move onto the neural network, let’s understand the dataset. This is a Kaggle dataset that uses GoodReads ratings of multiple users for ten thousand popular books. More information about the dataset can be found at https://www.kaggle.com/zygmunt/goodbooks-10k.\n",
    "\n",
    "The file books.csv contains metadata around the book like title, year of publishing, language, author among others. What we’d need are simply the title, language and ID from this file. Ratings.csv contains around 100 reviews for each book which range from one to five. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    book_id  user_id  rating\n",
      "0         1      314       5\n",
      "72        1    33890       3\n",
      "71        1    33872       5\n",
      "70        1    33716       5\n",
      "69        1    33697       4\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('Goodreads_Ratings/ratings.csv')\n",
    "books = pd.read_csv('Goodreads_Ratings/books.csv', usecols=['title', 'language_code', 'book_id', 'id'])\n",
    "\n",
    "books_rat_id = set(ratings.book_id)\n",
    "books = books[books.language_code.isin(['en', 'eng', 'en-CA', 'en-US', 'en-GB'])].sort_values(by=['title'])\n",
    "books_eng = list(books.id)\n",
    "# books_eng = books.sort_values(by=['title']).loc[books.language_code.isin(['en', 'eng', 'en-CA', 'en-US', 'en-GB']), 'id'].append(books.loc[books.language_code.isna(), 'id'])\n",
    "\n",
    "ratings = ratings.query(\"book_id in @books_eng\").sort_values(by=['book_id'])\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains much more information that would prove important to creating a user recommendation system, like the book’s author,  year of publishing etc. There’s even another file containing tags for the books and another which have books that have been marked as to-read by users. These have been excluded here, however, to stress only on the autoencoder’s ability to predict books the user might like simply based on the ones they have liked before. These can be added as features to the input vector after sufficient cleaning though. We also use only English books to add some consistency.\n",
    "\n",
    "Having many books with the same title is also an issue that we wish to solve by combining all ratings of the same book into the same bookID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate found for\n",
      " id                        349\n",
      "book_id                 11590\n",
      "title            'Salem's Lot\n",
      "language_code           en-GB\n",
      "Name: 348, dtype: object \n",
      "ID found: 4 \n",
      "\n",
      "Duplicate found for\n",
      " id                  6481\n",
      "book_id           384597\n",
      "title            Arcadia\n",
      "language_code      en-US\n",
      "Name: 6480, dtype: object \n",
      "ID found: 568 \n",
      "\n",
      "Duplicate found for\n",
      " id                              579\n",
      "book_id                      197084\n",
      "title            Are You My Mother?\n",
      "language_code                   eng\n",
      "Name: 578, dtype: object \n",
      "ID found: 573 \n",
      "\n",
      "Duplicate found for\n",
      " id                 3402\n",
      "book_id          739840\n",
      "title             Bambi\n",
      "language_code       eng\n",
      "Name: 3401, dtype: object \n",
      "ID found: 693 \n",
      "\n",
      "Duplicate found for\n",
      " id                                                    3846\n",
      "book_id                                           12283261\n",
      "title            Between the Lines (Between the Lines, #1)\n",
      "language_code                                          eng\n",
      "Name: 3845, dtype: object \n",
      "ID found: 838 \n",
      "\n",
      "Duplicate found for\n",
      " id                    7449\n",
      "book_id            6345193\n",
      "title            Invisible\n",
      "language_code          eng\n",
      "Name: 7448, dtype: object \n",
      "ID found: 3034 \n",
      "\n",
      "Duplicate found for\n",
      " id                    7462\n",
      "book_id                456\n",
      "title            Leviathan\n",
      "language_code          eng\n",
      "Name: 7461, dtype: object \n",
      "ID found: 3315 \n",
      "\n",
      "Duplicate found for\n",
      " id                  5480\n",
      "book_id            65684\n",
      "title            Monster\n",
      "language_code      en-US\n",
      "Name: 5479, dtype: object \n",
      "ID found: 3741 \n",
      "\n",
      "Duplicate found for\n",
      " id                  9813\n",
      "book_id            97408\n",
      "title            Perfect\n",
      "language_code        eng\n",
      "Name: 9812, dtype: object \n",
      "ID found: 4273 \n",
      "\n",
      "Duplicate found for\n",
      " id                                3112\n",
      "book_id                        7134202\n",
      "title            Private (Private, #1)\n",
      "language_code                      eng\n",
      "Name: 3111, dtype: object \n",
      "ID found: 4421 \n",
      "\n",
      "Duplicate found for\n",
      " id                         6455\n",
      "book_id                  112200\n",
      "title            Selected Poems\n",
      "language_code               eng\n",
      "Name: 6454, dtype: object \n",
      "ID found: 4843 \n",
      "\n",
      "Duplicate found for\n",
      " id                         8899\n",
      "book_id                  119239\n",
      "title            Selected Poems\n",
      "language_code               eng\n",
      "Name: 8898, dtype: object \n",
      "ID found: 4843 \n",
      "\n",
      "Duplicate found for\n",
      " id                   4457\n",
      "book_id          18225810\n",
      "title             Sisters\n",
      "language_code         eng\n",
      "Name: 4456, dtype: object \n",
      "ID found: 5002 \n",
      "\n",
      "Duplicate found for\n",
      " id                          1260\n",
      "book_id                     7510\n",
      "title            The Beach House\n",
      "language_code                eng\n",
      "Name: 1259, dtype: object \n",
      "ID found: 5575 \n",
      "\n",
      "Duplicate found for\n",
      " id                        5174\n",
      "book_id               18668066\n",
      "title            The Collector\n",
      "language_code              eng\n",
      "Name: 5173, dtype: object \n",
      "ID found: 5839 \n",
      "\n",
      "Duplicate found for\n",
      " id                             8168\n",
      "book_id                       47730\n",
      "title            The Complete Poems\n",
      "language_code                 en-US\n",
      "Name: 8167, dtype: object \n",
      "ID found: 5857 \n",
      "\n",
      "Duplicate found for\n",
      " id                               4939\n",
      "book_id                         22904\n",
      "title            The Complete Stories\n",
      "language_code                     eng\n",
      "Name: 4938, dtype: object \n",
      "ID found: 5865 \n",
      "\n",
      "Duplicate found for\n",
      " id                   7684\n",
      "book_id             59836\n",
      "title            The Gift\n",
      "language_code       en-US\n",
      "Name: 7683, dtype: object \n",
      "ID found: 6260 \n",
      "\n",
      "Duplicate found for\n",
      " id                    1174\n",
      "book_id           26893819\n",
      "title            The Girls\n",
      "language_code          eng\n",
      "Name: 1173, dtype: object \n",
      "ID found: 6289 \n",
      "\n",
      "Duplicate found for\n",
      " id                   7645\n",
      "book_id              2430\n",
      "title            The List\n",
      "language_code         eng\n",
      "Name: 7644, dtype: object \n",
      "ID found: 6732 \n",
      "\n",
      "Duplicate found for\n",
      " id                   8744\n",
      "book_id          23844390\n",
      "title            The Pact\n",
      "language_code         eng\n",
      "Name: 8743, dtype: object \n",
      "ID found: 7071 \n",
      "\n",
      "Duplicate found for\n",
      " id                   4212\n",
      "book_id          19101283\n",
      "title             The Son\n",
      "language_code         eng\n",
      "Name: 4211, dtype: object \n",
      "ID found: 7504 \n",
      "\n",
      "Duplicate found for\n",
      " id                        162\n",
      "book_id                 49552\n",
      "title            The Stranger\n",
      "language_code             eng\n",
      "Name: 161, dtype: object \n",
      "ID found: 7566 \n",
      "\n",
      "53424 8705\n",
      "          ID                       title user_id rating\n",
      "0          0   Angels (Walsh Family, #3)   22270      5\n",
      "1          0   Angels (Walsh Family, #3)   26945      5\n",
      "2          0   Angels (Walsh Family, #3)   26869      3\n",
      "3          0   Angels (Walsh Family, #3)   25518      3\n",
      "4          0   Angels (Walsh Family, #3)   24756      4\n",
      "...      ...                         ...     ...    ...\n",
      "859491  8705   ttyl (Internet Girls, #1)   14759      5\n",
      "859492  8705   ttyl (Internet Girls, #1)   14619      3\n",
      "859493  8705   ttyl (Internet Girls, #1)   14070      2\n",
      "859494  8705   ttyl (Internet Girls, #1)   16090      4\n",
      "859495  8705   ttyl (Internet Girls, #1)   13784      4\n",
      "\n",
      "[859496 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "eng_ratings_df = pd.DataFrame(columns = ['ID', 'title', 'user_id', 'rating'])\n",
    "id_ctr = 0\n",
    "\n",
    "for i, buk in books.iterrows():\n",
    "    buk_ratings = ratings[ratings['book_id'] == buk.id]\n",
    "    found_id = 0\n",
    "\n",
    "    if buk.title in list(eng_ratings_df.title):\n",
    "        found_id = eng_ratings_df[eng_ratings_df.title == buk.title].iloc[0, 0]  #To get the index of the first element\n",
    "        print(\"Duplicate found for\\n\", buk, \"\\nID found:\", found_id, \"\\n\")\n",
    "\n",
    "    for j, b_r in buk_ratings.iterrows():\n",
    "        eng_ratings_df = eng_ratings_df.append({'ID':int(id_ctr) if found_id == 0 else found_id,\n",
    "            'title':buk.title, 'user_id':b_r.user_id, 'rating':b_r.rating}, ignore_index=True)\n",
    "\n",
    "    id_ctr = id_ctr + (1 if found_id == 0 else 0)\n",
    "    \n",
    "n_users = int(max(eng_ratings_df.user_id))\n",
    "n_books = int(max(eng_ratings_df.ID)) + 1\n",
    "\n",
    "print(n_users, n_books)\n",
    "print(eng_ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will need to combine the books and ratings as picking only English books will lead to issues with indexing while training the model. Moreover, some books were observed to be missing in books.csv but present in ratings.csv which can be excluded only by the creation of a new index. This will also make future prediction easier where we would have to take ratings from users through a web application and figure out the correct order to send them as input to the autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure eng_ratings_df contains only English books with their names sorted and all the ratings for it from the ratings dataframe. We store this in a file (eng_books_ratings) to ensure we don’t have to process all the datasets again and again each time we wish to play with the autoencoder. Another file (eng_books_sorted.csv) contains just the books we have selected, in order to make it quicker for the flask application to retrieve and display on the HTML form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(eng_ratings_df.title.unique()).to_csv('eng_books_sorted.csv')\n",
    "eng_ratings_df.to_csv('eng_books_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final data structure (eng_usr_ratings) to store the ratings will be a 2D array with each column representing a book and each row representing a list of ratings by a user for some of the books. For compatibility we use a torch tensor eng_usr_ratings to store the above. We proceed with a 80-20 split for the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "42739 Length of training test:  42739  test set:  10685\n"
     ]
    }
   ],
   "source": [
    "eng_usr_ratings = torch.zeros([n_users, n_books], dtype=torch.float32)\n",
    "\n",
    "for i, rating_row in eng_ratings_df.iterrows():\n",
    "    eng_usr_ratings[int(rating_row.user_id)-1][int(rating_row.ID)-1] = rating_row.rating\n",
    "\n",
    "print(eng_usr_ratings[:5])\n",
    "\n",
    "lim = int(n_users * 0.8)\n",
    "tr_set = eng_usr_ratings[:lim][:]\n",
    "te_set = eng_usr_ratings[lim:][:]\n",
    "\n",
    "print(lim, \"Length of training test: \", tr_set.shape[0], \" test set: \", te_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create an object of SAE class that we'll use to create an autoencoder. We pass the no of books to the module to create the number of nodes in the input layer. Next, we set an optimizer that uses mean squared error as loss and for calculating weights, it uses a learning rate of 0.04 and weight decay of 0.005.\n",
    "\n",
    "These values have been selected through a trial and error approach, where other contenders were 0.05/0.008, 0.02/0.008 among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE(n_books=n_books)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.05, weight_decay=0.008)\n",
    "#Epoch:  50  Loss:  28184.567553707646  usrs:  41414.0  Avg Loss:  0.6805565160020197, TESTLOSS:0.9455141043192761\n",
    "\n",
    "# optimizer = optim.RMSprop(sae.parameters(), lr=0.04, weight_decay=0.005)\n",
    "#Epoch:  8  Loss:  36309.51346146319  usrs:  41414.0  Avg Loss:  0.8767449041740278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this autoencoder will take many steps.\n",
    "\n",
    "To begin with, we choose 100/50 as number of epochs. Using 100 causes overfitting and hence a smaller value is used here.\n",
    "For each epoch we have training loss (train_loss) and usrs, to keep track of the training loss and number of users with more than zero ratings. train_loss will be divided by usrs to get the average loss per epoch. It may be possible that we encounter rows corresponding to a user's ratings that have no non-zero value, i.e that user gave no ratings for the books we're interested in, since we have filtered out many books. Such rows will impair the calculation of average loss, since using the length of the training set will count these users too.\n",
    "\n",
    "We employ a gradient descent approach by going through the training set one by one and training the model using one user at a time. Since we're creating an autoencoder, the output (or target) is the same as the input given.\n",
    "\n",
    "Each row will go through a check to ensure that it contains at least one non-zero rating, and will increment usrs if it does. Based on the current weights and parameters of the autoencoder, we get the output and compare it to the actual target. For all the input values that were already 0, i.e. books that weren't rated by that particular user, we set the output element to 0, since we wouldnt need to calculate the loss against these values. To ensure that gradient descent isnt applied to the target variable like the weights, we set requires_grad as False.\n",
    "\n",
    "The loss is calculated against the calculated output and target variable and the next step instructs it to propagate backward. In order to correctly calculate the training loss, we count the number of ratings that are non-zero and use it as a mean corrector, square root it's product with the loss value, and add it to the training loss accumulated thus far. Next we call the optimizer we had declared earlier and call step() on it to change the weights using the learning rate and weight decay passed as params lr, weight_decay earlier.\n",
    "\n",
    "For each epoch we print out the training loss and the average value of it. For this model, we see it starts from 1.116 and drops down to 0.680 later. This shows that the model does learn significantly over time, and we can expect a difference of 0.68 between the actual and the real rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  46245.95948159071  usrs:  41414.0  Avg Loss:  1.1166745419807482\n",
      "Epoch:  2  Loss:  39498.6450982867  usrs:  41414.0  Avg Loss:  0.9537510285962888\n",
      "Epoch:  3  Loss:  38480.26008338464  usrs:  41414.0  Avg Loss:  0.9291606723181687\n",
      "Epoch:  4  Loss:  37750.32420220691  usrs:  41414.0  Avg Loss:  0.9115353311007609\n",
      "Epoch:  5  Loss:  37143.611114910316  usrs:  41414.0  Avg Loss:  0.8968853797003505\n",
      "Epoch:  6  Loss:  36566.0572917775  usrs:  41414.0  Avg Loss:  0.8829395202534771\n",
      "Epoch:  7  Loss:  36057.929476698395  usrs:  41414.0  Avg Loss:  0.8706700506277683\n",
      "Epoch:  8  Loss:  35592.13275662691  usrs:  41414.0  Avg Loss:  0.8594227255668834\n",
      "Epoch:  9  Loss:  35206.384903446145  usrs:  41414.0  Avg Loss:  0.8501082943798267\n",
      "Epoch:  10  Loss:  34797.22016826099  usrs:  41414.0  Avg Loss:  0.8402284292331335\n",
      "Epoch:  11  Loss:  34410.32627953615  usrs:  41414.0  Avg Loss:  0.8308863253860084\n",
      "Epoch:  12  Loss:  34087.54692769816  usrs:  41414.0  Avg Loss:  0.8230923583256425\n",
      "Epoch:  13  Loss:  33781.042953889446  usrs:  41414.0  Avg Loss:  0.8156913834425423\n",
      "Epoch:  14  Loss:  33463.43961368263  usrs:  41414.0  Avg Loss:  0.8080223985532099\n",
      "Epoch:  15  Loss:  33210.81468080466  usrs:  41414.0  Avg Loss:  0.8019224098325363\n",
      "Epoch:  16  Loss:  32944.61756382128  usrs:  41414.0  Avg Loss:  0.7954947014010064\n",
      "Epoch:  17  Loss:  32712.657025063207  usrs:  41414.0  Avg Loss:  0.7898936839006907\n",
      "Epoch:  18  Loss:  32485.739819951552  usrs:  41414.0  Avg Loss:  0.7844144448725443\n",
      "Epoch:  19  Loss:  32256.78683855532  usrs:  41414.0  Avg Loss:  0.7788860491272352\n",
      "Epoch:  20  Loss:  32112.75130259684  usrs:  41414.0  Avg Loss:  0.7754081060172126\n",
      "Epoch:  21  Loss:  31929.526113679003  usrs:  41414.0  Avg Loss:  0.7709838729337665\n",
      "Epoch:  22  Loss:  31698.110763032597  usrs:  41414.0  Avg Loss:  0.7653960197767082\n",
      "Epoch:  23  Loss:  31444.851940793178  usrs:  41414.0  Avg Loss:  0.7592807248947984\n",
      "Epoch:  24  Loss:  31231.889401108976  usrs:  41414.0  Avg Loss:  0.7541384411336499\n",
      "Epoch:  25  Loss:  31102.93285707996  usrs:  41414.0  Avg Loss:  0.7510246017549611\n",
      "Epoch:  26  Loss:  30924.831061791847  usrs:  41414.0  Avg Loss:  0.7467240803059798\n",
      "Epoch:  27  Loss:  30810.766994811336  usrs:  41414.0  Avg Loss:  0.743969840991243\n",
      "Epoch:  28  Loss:  30682.84763209562  usrs:  41414.0  Avg Loss:  0.7408810458322215\n",
      "Epoch:  29  Loss:  30574.290497942595  usrs:  41414.0  Avg Loss:  0.7382597792520065\n",
      "Epoch:  30  Loss:  30452.128448502783  usrs:  41414.0  Avg Loss:  0.7353100026199542\n",
      "Epoch:  31  Loss:  30187.886078679017  usrs:  41414.0  Avg Loss:  0.7289294943419862\n",
      "Epoch:  32  Loss:  30092.74214362951  usrs:  41414.0  Avg Loss:  0.7266321085533759\n",
      "Epoch:  33  Loss:  29967.42798619696  usrs:  41414.0  Avg Loss:  0.7236062197855063\n",
      "Epoch:  34  Loss:  29680.028287206536  usrs:  41414.0  Avg Loss:  0.7166665448207499\n",
      "Epoch:  35  Loss:  29639.323675990578  usrs:  41414.0  Avg Loss:  0.7156836740230497\n",
      "Epoch:  36  Loss:  29530.44631139775  usrs:  41414.0  Avg Loss:  0.7130546750228848\n",
      "Epoch:  37  Loss:  29448.277159976533  usrs:  41414.0  Avg Loss:  0.7110705838599636\n",
      "Epoch:  38  Loss:  29471.003380593193  usrs:  41414.0  Avg Loss:  0.7116193408169507\n",
      "Epoch:  39  Loss:  29417.751605041198  usrs:  41414.0  Avg Loss:  0.710333500870266\n",
      "Epoch:  40  Loss:  29365.006741730653  usrs:  41414.0  Avg Loss:  0.709059901041451\n",
      "Epoch:  41  Loss:  29197.799680940883  usrs:  41414.0  Avg Loss:  0.7050224484701039\n",
      "Epoch:  42  Loss:  29055.827223525583  usrs:  41414.0  Avg Loss:  0.701594321329154\n",
      "Epoch:  43  Loss:  28982.003978745342  usrs:  41414.0  Avg Loss:  0.6998117539659376\n",
      "Epoch:  44  Loss:  28908.269974743904  usrs:  41414.0  Avg Loss:  0.6980313414483967\n",
      "Epoch:  45  Loss:  28733.883875453066  usrs:  41414.0  Avg Loss:  0.6938205407701035\n",
      "Epoch:  46  Loss:  28552.19510652491  usrs:  41414.0  Avg Loss:  0.6894334067350391\n",
      "Epoch:  47  Loss:  28467.67289645458  usrs:  41414.0  Avg Loss:  0.6873924976204805\n",
      "Epoch:  48  Loss:  28351.34320699967  usrs:  41414.0  Avg Loss:  0.6845835516250464\n",
      "Epoch:  49  Loss:  28314.275844724943  usrs:  41414.0  Avg Loss:  0.6836885073821641\n",
      "Epoch:  50  Loss:  28184.567553707646  usrs:  41414.0  Avg Loss:  0.6805565160020197\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 50 #100\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    train_loss = 0.                                                     # Calculates training loss in one epoch\n",
    "    usrs = 0.                                                           # Counts no of rows (users) with > 0 nonzero ratings\n",
    "\n",
    "    for i in range(len(tr_set)):\n",
    "        input = Variable(tr_set[i]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        \n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            usrs += 1\n",
    "            \n",
    "            output = sae(input)\n",
    "            output[target == 0] = 0\n",
    "            target.requires_grad = False\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            mean_corrector = n_books / float(torch.sum(target.data > 0) + 1e-10)\n",
    "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "    print('Epoch: ', ep+1, ' Loss: ', train_loss, ' usrs: ', usrs, ' Avg Loss: ', train_loss/usrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model created involves many of the exact same steps. We start off by creating test_loss and usrs like before, which will calculate the testing loss and number of users with more than zero ratings. \n",
    "\n",
    "Again, the input and the target are clones of each other, and we disregard the ratings predicted by SAE for books that the user didnt originally rate (in input) by setting the corresponding values in output to zero. \n",
    "The loss is calculated using the same criterion as before but we ensure that we dont let it propagate backwards or call optimizer.step() to shift weights. We simply calculate the mean corrector, use it to calculate the loss and add it to the test loss obtained thus far.\n",
    "\n",
    "We print the testing loss and since we see that its 0.945, we can conclude that we have an issue of overfitting in our autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  9763.378641200845  usrs:  10326.0 Avg Test Loss:  0.9455141043192761\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.                                                          # Calculates test loss\n",
    "usrs = 0.                                                               # Counts no of rows (users) with >= 1 nonzero ratings\n",
    "\n",
    "for i in range(len(te_set)):\n",
    "    input = Variable(te_set[i]).unsqueeze(0)\n",
    "    target = input.clone()\n",
    "    \n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        usrs += 1\n",
    "        \n",
    "        output = sae(input)\n",
    "        output[target.data == 0] = 0\n",
    "        target.requires_grad = False\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        mean_corrector = n_books / float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "        \n",
    "print('Test Loss: ', test_loss, ' usrs: ', usrs, 'Avg Test Loss: ', test_loss/usrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained Stacked AutoEncoder ready to predict ratings, we have to make sure that we store it somewhere so it doesn’t have be initialized every time we want to make a prediction. Hence, we pickle it to store into a new file trainedSae. The flask application only needs to import this once and use for every prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('trainedSae','wb') as outf:\n",
    "    pickle.dump(sae, outf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we read from the same pickled file, and we see how we can predict the ratings for a randomly created user.\n",
    "This pickled file will be used in our web application to make predictions on ratings given by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
