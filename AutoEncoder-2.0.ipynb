{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a model which uses Tensorflow's Keras Model to create an autoencoder. Autoencoders are used to replicate the input we feed into the network by encoding and decoding the input multiple times. Refer to Autoencoder.py for the implementation used here. \n",
    "\n",
    "It has layers in the following format:\n",
    "\n",
    "1. Input: Same number of nodes as number of books\n",
    "2. Hidden1: 80 nodes to encode the input once\n",
    "3. Hidden2: 40 nodes to encode the input twice\n",
    "4. Hidden3: 80 nodes to decode the input once\n",
    "5. Hidden4: Output layer, decodes twice to bring back number of dimensions same as number of books\n",
    " \n",
    "We start by importing all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhead\\.conda\\envs\\playplace\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\rhead\\.conda\\envs\\playplace\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\rhead\\.conda\\envs\\playplace\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy and pndas are going to help us out with arithmetic and for handling dataframes, respectively. Tensorflow will be for creating out autoencoder, training it and testing the results.\n",
    "And the last line imports the Stacked AutoEncoder class we’ve created. If we prefer we can move the training logic into the class itself in the form of class methods but in order to experiment with optimizers and training logic, it can be left here.\n",
    "\n",
    "Before we move onto the neural network, let’s understand the dataset. This is a Kaggle dataset that uses GoodReads ratings of multiple users for ten thousand popular books. More information about the dataset can be found at https://www.kaggle.com/zygmunt/goodbooks-10k.\n",
    "\n",
    "The file books.csv contains metadata around the book like title, year of publishing, language, author among others. What we’d need are simply the title, language and ID from this file. Ratings.csv contains around 100 reviews for each book which range from one to five. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    book_id  user_id  rating\n",
      "0         1      314       5\n",
      "72        1    33890       3\n",
      "71        1    33872       5\n",
      "70        1    33716       5\n",
      "69        1    33697       4\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('Goodreads_Ratings/ratings.csv')\n",
    "books = pd.read_csv('Goodreads_Ratings/books.csv', usecols=['title', 'language_code', 'book_id', 'id'])\n",
    "\n",
    "books_rat_id = set(ratings.book_id)\n",
    "books = books[books.language_code.isin(['en', 'eng', 'en-CA', 'en-US', 'en-GB'])].sort_values(by=['title'])\n",
    "books_eng = list(books.id)\n",
    "# books_eng = books.sort_values(by=['title']).loc[books.language_code.isin(['en', 'eng', 'en-CA', 'en-US', 'en-GB']), 'id'].append(books.loc[books.language_code.isna(), 'id'])\n",
    "\n",
    "ratings = ratings.query(\"book_id in @books_eng\").sort_values(by=['book_id'])\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains much more information that would prove important to creating a user recommendation system, like the book’s author,  year of publishing etc. There’s even another file containing tags for the books and another which have books that have been marked as to-read by users. These have been excluded here, however, to stress only on the autoencoder’s ability to predict books the user might like simply based on the ones they have liked before. These can be added as features to the input vector after sufficient cleaning though. We also use only English books to add some consistency.\n",
    "\n",
    "Having many books with the same title is also an issue that we wish to solve by combining all ratings of the same book into the same bookID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_ratings_df = pd.DataFrame(columns = ['ID', 'title', 'user_id', 'rating'])\n",
    "id_ctr = 0\n",
    "\n",
    "for i, buk in books.iterrows():\n",
    "    buk_ratings = ratings[ratings['book_id'] == buk.id]\n",
    "    found_id = 0\n",
    "\n",
    "    if buk.title in list(eng_ratings_df.title):\n",
    "        found_id = eng_ratings_df[eng_ratings_df.title == buk.title].iloc[0, 0]  #To get the index of the first element\n",
    "        print(\"Duplicate found for\\n\", buk, \"\\nID found:\", found_id, \"\\n\")\n",
    "\n",
    "    for j, b_r in buk_ratings.iterrows():\n",
    "        eng_ratings_df = eng_ratings_df.append({'ID':int(id_ctr) if found_id == 0 else found_id,\n",
    "            'title':buk.title, 'user_id':b_r.user_id, 'rating':b_r.rating}, ignore_index=True)\n",
    "\n",
    "    id_ctr = id_ctr + (1 if found_id == 0 else 0)\n",
    "    \n",
    "n_users = int(max(eng_ratings_df.user_id))\n",
    "n_books = int(max(eng_ratings_df.ID)) + 1\n",
    "\n",
    "print(n_users, n_books)\n",
    "print(eng_ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will need to combine the books and ratings as picking only English books will lead to issues with indexing while training the model. Moreover, some books were observed to be missing in books.csv but present in ratings.csv which can be excluded only by the creation of a new index. This will also make future prediction easier where we would have to take ratings from users through a web application and figure out the correct order to send them as input to the autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure eng_ratings_df contains only English books with their names sorted and all the ratings for it from the ratings dataframe. We store this in a file (eng_books_ratings) to ensure we don’t have to process all the datasets again and again each time we wish to play with the autoencoder. Another file (eng_books_sorted.csv) contains just the books we have selected, in order to make it quicker for the flask application to retrieve and display on the HTML form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(eng_ratings_df.title.unique()).to_csv('eng_books_sorted.csv')\n",
    "eng_ratings_df.to_csv('eng_books_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final data structure (eng_usr_ratings) to store the ratings will be a 2D array with each column representing a book and each row representing a list of ratings by a user for some of the books. For compatibility we use a torch tensor eng_usr_ratings to store the above. We proceed with a 80-20 split for the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "eng_ratings_df = pd.read_csv('eng_books_ratings.csv')\n",
    "n_users = int(max(eng_ratings_df.user_id))\n",
    "n_books = int(max(eng_ratings_df.ID)) + 1\n",
    "\n",
    "eng_usr_ratings = np.zeros([n_users, n_books], dtype=np.float32)\n",
    "\n",
    "for i, rating_row in eng_ratings_df.iterrows():\n",
    "    eng_usr_ratings[int(rating_row.user_id)-1][int(rating_row.ID)-1] = rating_row.rating\n",
    "\n",
    "# eng_usr_ratings = tf.convert_to_tensor(eng_usr_ratings, dtype=tf.float32)\n",
    "print(eng_usr_ratings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53424 Length of training test:  42739  test set:  10685\n"
     ]
    }
   ],
   "source": [
    "lim = int(n_users*0.80)\n",
    "tr_set = eng_usr_ratings[:lim][:]\n",
    "te_set = eng_usr_ratings[lim:][:]\n",
    "\n",
    "print(n_users, \"Length of training test: \", tr_set.shape[0], \" test set: \", te_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our own RMS error function for the network.\n",
    "Our data might have rows with some values all set to zero, as they represent books that the user hasn't read yet. On an average, each user will have only 3-5 ratings, and the rest of the columns would be 0.\n",
    "As a result, we don't need to consider the predicted values of these ratings in our function as they should always be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rmse(y_true, y_pred):\n",
    "    nonzero = (y_true != 0)\n",
    "    nonzero = tf.dtypes.cast(nonzero, tf.float32)\n",
    "    \n",
    "    y_new = y_pred * nonzero\n",
    "    \n",
    "    error = y_true-y_new\n",
    "    sqr_error = K.square(error)\n",
    "    mean_sqr_error = K.mean(sqr_error)\n",
    "    sqrt_mean_sqr_error = K.sqrt(mean_sqr_error)\n",
    "    \n",
    "    return sqrt_mean_sqr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create an object of Autoencoder class that we'll use to create our neural network. We pass the number of books to the module to create the number of nodes in the input layer. Next, we set an optimizer that uses the mean squared error function we just defined. The batch_size is set to 128, instead of the default 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "334/334 [==============================] - 8s 19ms/step - loss: 0.1135 - val_loss: 0.0677\n",
      "Epoch 2/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0709 - val_loss: 0.0666\n",
      "Epoch 3/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0693 - val_loss: 0.0654\n",
      "Epoch 4/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0676 - val_loss: 0.0648\n",
      "Epoch 5/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0663 - val_loss: 0.0636\n",
      "Epoch 6/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0645 - val_loss: 0.0624\n",
      "Epoch 7/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0634 - val_loss: 0.0612\n",
      "Epoch 8/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 9/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0596 - val_loss: 0.0579\n",
      "Epoch 10/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0579 - val_loss: 0.0570\n",
      "Epoch 11/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0562 - val_loss: 0.0556\n",
      "Epoch 12/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0542 - val_loss: 0.0533\n",
      "Epoch 13/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0522 - val_loss: 0.0522\n",
      "Epoch 14/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0507 - val_loss: 0.0513\n",
      "Epoch 15/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0489 - val_loss: 0.0492\n",
      "Epoch 16/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0476 - val_loss: 0.0495\n",
      "Epoch 17/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0466 - val_loss: 0.0491\n",
      "Epoch 18/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0460 - val_loss: 0.0474\n",
      "Epoch 19/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0452 - val_loss: 0.0477\n",
      "Epoch 20/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0444 - val_loss: 0.0465\n",
      "Epoch 21/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0437 - val_loss: 0.0463\n",
      "Epoch 22/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0430 - val_loss: 0.0457\n",
      "Epoch 23/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0417 - val_loss: 0.0453\n",
      "Epoch 24/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0415 - val_loss: 0.0451\n",
      "Epoch 25/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0410 - val_loss: 0.0447\n",
      "Epoch 26/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0406 - val_loss: 0.0445\n",
      "Epoch 27/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0405 - val_loss: 0.0440\n",
      "Epoch 28/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0397 - val_loss: 0.0439\n",
      "Epoch 29/50\n",
      "334/334 [==============================] - 4s 12ms/step - loss: 0.0397 - val_loss: 0.0438\n",
      "Epoch 30/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0390 - val_loss: 0.0437\n",
      "Epoch 31/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0388 - val_loss: 0.0436\n",
      "Epoch 32/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0381 - val_loss: 0.0435\n",
      "Epoch 33/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0378 - val_loss: 0.0434\n",
      "Epoch 34/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0374 - val_loss: 0.0431\n",
      "Epoch 35/50\n",
      "334/334 [==============================] - 4s 12ms/step - loss: 0.0373 - val_loss: 0.0430\n",
      "Epoch 36/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0369 - val_loss: 0.0430\n",
      "Epoch 37/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0365 - val_loss: 0.0428\n",
      "Epoch 38/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0371 - val_loss: 0.0427\n",
      "Epoch 39/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0363 - val_loss: 0.0424\n",
      "Epoch 40/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0360 - val_loss: 0.0424\n",
      "Epoch 41/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0362 - val_loss: 0.0426\n",
      "Epoch 42/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0360 - val_loss: 0.0421\n",
      "Epoch 43/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0358 - val_loss: 0.0423\n",
      "Epoch 44/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0358 - val_loss: 0.0422\n",
      "Epoch 45/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0353 - val_loss: 0.0423\n",
      "Epoch 46/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0354 - val_loss: 0.0422\n",
      "Epoch 47/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0352 - val_loss: 0.0423\n",
      "Epoch 48/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0353 - val_loss: 0.0423\n",
      "Epoch 49/50\n",
      "334/334 [==============================] - 4s 13ms/step - loss: 0.0350 - val_loss: 0.0421\n",
      "Epoch 50/50\n",
      "334/334 [==============================] - 4s 12ms/step - loss: 0.0350 - val_loss: 0.0422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5b1f7f040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = Autoencoder(n_books)\n",
    "autoencoder.compile(optimizer='adam', loss=my_rmse)\n",
    "autoencoder.fit(tr_set, tr_set, epochs=50, shuffle=True, batch_size=128, validation_data=(te_set, te_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\rhead\\PycharmProjects\\autoEncoderRecommender\\tensorflowSAE\\assets\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save(r\"C:\\Users\\rhead\\PycharmProjects\\autoEncoderRecommender\\tensorflowSAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained autoencoder is saved to be called again whenever we need it to make predictions based on the user's inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
